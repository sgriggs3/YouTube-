import os

# Define the directory to search for API key log files
directory = "/workspaces/Fix-my-prebui21YouTube-Insight-Analyzer-Enhanced"

# Define the output file
output_file = os.path.join(directory, "all_api_keys.txt")

# List of log files to search for API keys
log_files = [
    "gemini_api_keys.log",
    "anthropic_keys.json",
    "lmstudio_keys.json",
    "openai_keys.json",
    "bedrock_keys.json",
    "vertex_keys.json"
]

# Function to read API keys from a file
def read_api_keys(file_path):
    try:
        with open(file_path, "r") as file:
            return file.readlines()
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
        return []

# Collect all API keys
all_api_keys = []
for log_file in log_files:
    file_path = os.path.join(directory, log_file)
    all_api_keys.extend(read_api_keys(file_path))

# Save all API keys to the output file
with open(output_file, "w") as file:
    for key in all_api_keys:
        file.write(key)

print(f"All API keys have been saved to {output_file}")

# Cline Session 1 Instructions: Backend Development

## Focus
This Cline session will focus on the backend development of the YouTube Insight Analyzer project.

## Tasks
1.  **YouTube API Integration:**
    *   Implement functions to fetch video metadata (title, description, tags, etc.).
    *   Implement functions to fetch comments for a given video.
    *   Implement functions to fetch video transcripts (if available).
    *   Handle API rate limits and errors gracefully.
    *   Store fetched data in a structured format.
    *   Use the `youtube_api.py` file to implement the YouTube API integration.
2.  **Sentiment Analysis:**
    *   Implement sentiment analysis on video comments.
    *   Implement sentiment analysis on video transcripts.
    *   Use a suitable NLP library (e.g., NLTK, spaCy, transformers).
    *   Store sentiment scores along with the corresponding text.
    *   Use the `sentiment_analysis.py` file to implement the sentiment analysis functionality.
3.  **Data Visualization:**
    *   Create functions to generate charts and graphs from the analysis data.
    *   Use a suitable data visualization library (e.g., matplotlib, seaborn, plotly).
    *   Implement visualizations for sentiment scores, comment frequency, etc.
    *   Use the `data_visualization.py` file to implement the data visualization functionality.
4.  **Data Storage:**
    *   Choose a suitable data storage mechanism (e.g., SQLite, PostgreSQL, JSON files).
    *   Implement functions to store and retrieve data.
    *   Ensure data integrity and consistency.
5.  **API Endpoints:**
    *   Create API endpoints for fetching video metadata.
    *   Create API endpoints for fetching sentiment analysis results.
    *   Create API endpoints for fetching data visualizations.
    *   Use a suitable web framework (e.g., Flask, FastAPI).
    *   Implement proper error handling and response codes.

## Important Notes
*   Ensure that all code is well-documented and follows best practices.
*   Coordinate with Cline session 2 to ensure that the API endpoints are compatible with the web UI.
*   Be aware of the testing and deployment tasks, but they are not the primary focus of this session.
